{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from csv import reader\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def load_csv(filename):\n",
    "    file = pd.read_csv(filename)\n",
    "    lines = reader(file)\n",
    "    dataset = list(lines)\n",
    "    return dataset\n",
    "\n",
    "def train_test_split(dataset, split=0.80):\n",
    "    train = list()\n",
    "    train_size = split * len(dataset)\n",
    "    dataset_copy = list(dataset)\n",
    "    while len(train) < train_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        train.append(dataset_copy.pop(index))\n",
    "    return train, dataset_copy\n",
    "\n",
    "\n",
    "def cross_valdation_split(dataset,n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset)/ n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold)< fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i]== predicted[i]:\n",
    "            correct +=1\n",
    "    return correct / float(len(actual)) *100.0\n",
    "\n",
    "def evaluate_algorithm_single(dataset,algorithm,*args):\n",
    "    train_set,test_set = train_test_split(dataset,algorithm,0.80)\n",
    "    scores = list()\n",
    "    predicted = algorithm(train_set,test_set, *args)\n",
    "    actual = [row[-1] for row in test_set]\n",
    "    accuracy = accuracy_metric(actual,predicted)\n",
    "    scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "def evaluate_algorithm(dataset,algorithm,n_folds,*args):\n",
    "    folds = cross_validation_split(dataset,n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set,[])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1]= None\n",
    "        predicted = algorithm(train_set,test_set,*args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual,predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split(index,value,dataset):\n",
    "    left,right = list(),list()\n",
    "    for row in dataset:\n",
    "        if row[index]<value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Gini Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(groups,classes):\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size ==0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val)/size\n",
    "            score +=p*p\n",
    "        gini +=(1.0 -score)*(size/n_instances)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(groups,classes,b_score):\n",
    "    n_instances = float(sum([len(group)for group in groups]))\n",
    "    ent = 0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size ==0:\n",
    "            continue\n",
    "        size=0.0\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val)/size\n",
    "            \n",
    "        if p>0:\n",
    "            score = (p*math.log(p,2))\n",
    "        ent -=(score*(size/n_instances))\n",
    "    return ent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the Best split point for a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(dataset,split_parameter):\n",
    "    if split_parameter =='entropy':\n",
    "        class_values = list(set(row[-1] for row in dataset))\n",
    "        b_index, b_value, b_score,b_groups = 999,999,1,None\n",
    "        for index in range(len(dataset[0])-1):\n",
    "            for row in dataset:\n",
    "                groups = test_split(index,row[index],dataset)\n",
    "                ent = entropy(groups,class_values,b_score)\n",
    "                if ent < b_score:\n",
    "                    b_index,b_value,b_score,b_groups = index,row[index],ent,groups\n",
    "        return {'index':b_index,'value':b_value,'groups':b_groups}\n",
    "    elif split_parameter =='gini':\n",
    "        class_values = list(set(row[-1]for row in dataset))\n",
    "        b_index, b_value,b_score, b_groups = 999,999,1,None\n",
    "        for index in range(len(dataset[0])-1):\n",
    "            for row in dataset:\n",
    "                groups = test_split(index,row[index],dataset)\n",
    "                gini= gini_index(groups,class_values)\n",
    "                if gini < b_score:\n",
    "                    b_index, b_value, b_score, b_groups = index, row[index],gini,groups\n",
    "        return {'index':b_index,'value':b_value,'groups':b_groups}\n",
    "    \n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1]for row in group]\n",
    "    return max(set(outcomes),key= outcomes.count)\n",
    "\n",
    "\n",
    "def split(node,max_depth,min_size,depth):\n",
    "    left,right = node['groups']\n",
    "    del(node['groups'])\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left+right)\n",
    "        return\n",
    "    if depth >=max_depth:\n",
    "        node['left'],node['right'] = to_terminal(left),to_terminal(right)\n",
    "        return\n",
    "    if len(left) <=min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left,split_parameter)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    if len(right) <=min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    \n",
    "    else:\n",
    "        ode['right'] = get_split(right,split_parameter)\n",
    "        split(node['right'], max_depth, min_size, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(train,max_depth,min_size,split_parameter):\n",
    "    root = get_split(train,split_parameter)\n",
    "    split(root,max_depth,min_size,1)\n",
    "    return root\n",
    "\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node,dict):\n",
    "        print('%s[ATTRIBUTE[%s] = %.50s]' % ((depth*'\\t', (node['index']+1), node['value'])))\n",
    "        print_tree(node['left'],depth+1)\n",
    "        print_tree(node['right'],depth+1)\n",
    "    else:\n",
    "        print('%s[%s]'% ((depth*' ',node)))\n",
    "        \n",
    "def predict(node,row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'],dict):\n",
    "            return predict(node['left'],row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'],dict):\n",
    "            return predict(node['right'],row)\n",
    "        else:\n",
    "            return node['right']\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(train,test,max_depth,min_size,split_parameter):\n",
    "    tree = build_tree(train,max_depth,min_size,split_parameter)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(tree,row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-07bbeab99ae5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmin_size\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msplit_parameter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'entropy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit_parameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dictionary Representation of Tree on training set'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-dc58d63cad0d>\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(dataset, split)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdataset_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_copy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\random.py\u001b[0m in \u001b[0;36mrandrange\u001b[1;34m(self, start, stop, step, _int)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mistart\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"empty range for randrange()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;31m# stop argument supplied.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty range for randrange()"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "filename = \"house-votes-84.csv\"\n",
    "dataset = pd.read_csv(filename)\n",
    "n_folds = 5\n",
    "max_depth =3\n",
    "min_size =1\n",
    "split_parameter = 'entropy'\n",
    "train_set,test_set = train_test_split(dataset,0.80)\n",
    "tree = build_tree(train_set,max_depth,min_size,split_parameter)\n",
    "print('Dictionary Representation of Tree on training set')\n",
    "print('  ')\n",
    "print(tree)\n",
    "print('  ')\n",
    "print('Attributes')\n",
    "print(dataset[0])\n",
    "print('Textual Representation of JSON Tree')\n",
    "print_tree(tree)\n",
    "scores_1 = evaluate_algorithm_single(dataset,decision_tree,max_depth,min_size,split_parameter)\n",
    "print('  ')\n",
    "print('Implementing Single split')\n",
    "print('Scores: %s'% scores_1)\n",
    "print('Accuracy: %.3f%%' % (sum(scores_1)/float(len(scores_1))))\n",
    "\n",
    "print(' ')\n",
    "print('Implementing k-cross validation')\n",
    "scores_2 = evaluate_algorithm(dataset,decision_tree,max_depth,min_size,split_parameter)\n",
    "print('Scores: %s' % scores_2)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores_2)/float(len(scores_2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
